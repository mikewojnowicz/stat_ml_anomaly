\pdfoutput=1
\documentclass[10pt]{beamer}

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{../../rsrc/beamer_preamble}

% RANDOM VARIABLE
\newcommand{\x}{X}
\newcommand{\y}{Y}

\title{Independence Properties \\ of Directed Probabilistic Graphical Models}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Motivation}
\begin{frame}{Motivation}
Consider a model of interest; e.g.,
%\begin{figure}[H]
%  \centering
%  \scalebox{1.0}{
%  \tikz{ %
%    \node[latent] (beta) {$\beta$} ; %
%    \node[latent, below=of beta] (pi) {$\pi_k$} ; %
%    \node[latent, below=of pi] (theta) {$\theta_k$} ; %
%    \node[const, left=of beta] (gamma) {$\gamma$}; %
%    \node[const, left=of pi] (alpha) {$\alpha$}; %
%     \node[const, left=of theta] (H) {$H$}; %
% \plate[inner sep=0.25cm, xshift=0cm, yshift=0cm] {plate1} {(pi) (theta)} {$\infty$}; %
%     \node[obs, right=of pi](x0){$\x_0$}; %
%     \node[latent, right=of x0](x1){$\x_1$}; %
%     \node[latent, right=of x1](x2){$\x_2$};%
%     \node[const, right=of x2](ellipses){$\; \+\ldots \;$}; %
%     \node[latent, right=of ellipses](xT){$\x_T$}; %
%     \draw [->] (pi) to [out=30, in=150] (x1);
%     \draw [->] (pi) to [out=30, in=150] (x2);
%     \draw [->] (pi) to [out=30, in=150] (xT);
%     \node[obs, below=of x1](y1){$\y_1$}; %
%     \node[obs, right=of y1](y2){$\y_2$};%
%     \node[const, right=of y2](yellipses){$\; \+\ldots \;$}; %
%     \node[obs, right=of yellipses](yT){$\y_T$}; %
%     \draw [->] (theta) to [out=30, in=150] (y1);
%     \draw [->] (theta) to [out=30, in=150] (y2);
%     \draw [->] (theta) to [out=30, in=150] (yT);
% \edge{beta}{pi} ;
% \edge{gamma}{beta} ;
% \edge{alpha}{pi} ;
%  \edge{pi}{theta} ;
% \edge{H}{theta} ;
% \edge{x0}{x1};
% \edge{x1}{x2};
%  \edge{x2}{ellipses};
%  \edge{ellipses}{xT};
%  \edge{x1}{y1};
%   \edge{x2}{y2};
%  \edge{xT}{yT};
% \edge{y1}{y2};
%  \edge{y2}{yellipses};
%  \edge{yellipses}{yT};
%  }
% }
%\caption{The HDP-HMM as a graphical model}
%\label{fig:HDP_HMM_graphical_model}
%\end{figure}

\begin{figure}[H]
  \centering
  \scalebox{1.0}{
  \tikz{ %
    \node[latent] (pi) {$\pi_k$} ; %
    \node[latent, below=of pi] (theta) {$\theta_k$} ; %
    \node[const, left=of pi] (alpha) {$\alpha$}; %
     \node[const, left=of theta] (H) {$H$}; %
 \plate[inner sep=0.25cm, xshift=0cm, yshift=0cm] {plate1} {(pi) (theta)} {$K$}; %
     \node[obs, right=of pi](x0){$\x_0$}; %
     \node[latent, right=of x0](x1){$\x_1$}; %
     \node[latent, right=of x1](x2){$\x_2$};%
     \node[const, right=of x2](ellipses){$\; \+\ldots \;$}; %
     \node[latent, right=of ellipses](xT){$\x_T$}; %
     \draw [->] (pi) to [out=30, in=150] (x1);
     \draw [->] (pi) to [out=30, in=150] (x2);
     \draw [->] (pi) to [out=30, in=150] (xT);
     \node[obs, below=of x1](y1){$\y_1$}; %
     \node[obs, right=of y1](y2){$\y_2$};%
     \node[const, right=of y2](yellipses){$\; \+\ldots \;$}; %
     \node[obs, right=of yellipses](yT){$\y_T$}; %
     \draw [->] (theta) to [out=30, in=150] (y1);
     \draw [->] (theta) to [out=30, in=150] (y2);
     \draw [->] (theta) to [out=30, in=150] (yT);
 \edge{alpha}{pi} ;
  \edge{pi}{theta} ;
 \edge{H}{theta} ;
 \edge{x0}{x1};
 \edge{x1}{x2};
  \edge{x2}{ellipses};
  \edge{ellipses}{xT};
  \edge{x1}{y1};
   \edge{x2}{y2};
  \edge{xT}{yT};
 \edge{y1}{y2};
  \edge{y2}{yellipses};
  \edge{yellipses}{yT};
  }
 }
\caption{The Bayesian HMM as a graphical model}
\label{fig:bayesian_HMM_graphical_model}
\end{figure}
How can we easily answer queries about \tiny (conditional \textit{or} marginal) \normalsize independence ?
\end{frame}

\section{Directed Probabilistic  Graphical Models }
\begin{frame}{Joint distributions}
The starting point for a directed probabilistic graphical model is a particular factorization of a joint density:

\begin{equation}
\label{eqn:joint_factorization}
 p(\x_1, ..., \x_n) = \ds\prod_{i=1}^n p(\x_i \cond \pi_i ) 
\end{equation}

where the conditioning set $\pi_i$ is referred to as the \alert{parents} of  variable $i$.

\eqref{eqn:joint_factorization} simplifies the factorizations which are \textit{always} true, by the chain rule of probability:

\[ p(\x_1, ..., \x_n) = \ds\prod_{i=1}^n p(\x_i \cond \x_1, .., \x_{i-1} ) \]

\vfill \vfill
\tiny In other words, \eqref{eqn:joint_factorization} restricts our consideration to a certain subset of joint probability distributions.  \\

\tiny Consider, e.g., the structure imposed in Bayesian models by independent priors or conditionally i.i.d likelihoods.  \normalsize

\end{frame}

%The problem is that it is not clear in advance that the arbitrary collection of conditionals given in \eqref{eqn:joint_factorization} is internally consistent, and are indeed the conditionals for the provided joint.  Luckily, there is a theorem (not proven here) which guarantees this to be true.\footnote{This may be demonstrated in Chapter 3 of \cite{koller}; I'm not sure.}

\begin{frame}{Directed probabilistic graphical models} 
\footnotesize 
Once we have specified our desired factorization via \eqref{eqn:joint_factorization}, we can identify it with a directed acyclic graph (DAG) $\mathcal{G} = (E,V)$ by:
\begin{itemize}
\item identifying each random variable with a node
\item drawing a directed arc from $A$ to $B$ if $A$ is a parent of $B$ 
\end{itemize}

 We call this representation a \alert{directed probabilistic graphical model}  \tiny (or a Bayesian network)  \footnotesize.
 
 For example, the DAG in Figure \ref{fig:example_directed_pgm}

\begin{figure}[H]
  \centering
  \scalebox{0.7}{
  \tikz{ %
    \node[latent] (x1) at (0,0) {$\x_1$} ; %
    \node[latent] (x2) at (2,1) {$\x_2$} ; %
    \node[latent] (x3) at (2,-1) {$\x_3$} ; %
    \node[latent] (x4) at (4,2) {$\x_4$} ; %
    \node[latent] (x5) at (4,-1) {$\x_5$} ; %
    \node[latent] (x6) at (6,0) {$\x_6$} ; %
 \edge{x1}{x2};
 \edge{x1}{x3};
 \edge{x2}{x4};
  \edge{x2}{x6};
  \edge{x3}{x5};
  \edge{x5}{x6};
  }
 }
\caption{An example directed acyclic graph (DAG).}
\label{fig:example_directed_pgm}
\end{figure}

 corresponds to the factorization
\[ p(\x) = p(\x_1) \, p(\x_2 \cond \x_1) \, p(\x_3 \cond \x_1) \, p(\x_4 \cond \x_2) \, p(\x_5 \cond \x_3) \, p(\x_6 \cond \x_5, \x_2) \]
\end{frame}

\section{Independence in Canonical Graphs}

%\begin{frame}{Three canonical graphs}
%
%\begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%		  \centering
%		  \scalebox{0.7}{
%		  \tikz{ %
%		    \node[latent] (x) at (0,0) {$x$} ; %
%		    \node[latent] (y) at (0,-2) {$y$} ; %
%		    \node[latent] (z) at (0,-4) {$z$} ; %
%		 \edge{x}{y};
%		 \edge{y}{z};
%		 }
%	}
%     \caption{Cascade}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%		  \centering
%		  \scalebox{0.7}{
%		  \tikz{ %
%		    \node[latent] (x) at (0,0) {$x$} ; %
%		    \node[latent] (y) at (-2,-2) {$y$} ; %
%		    \node[latent] (z) at (2,-2) {$z$} ; %
%		 \edge{x}{y};
%		 \edge{x}{z};
%		 }
%	}
%     \caption{Common parent}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%		  \centering
%		  \scalebox{0.7}{
%		  \tikz{ %
%		    \node[latent] (x) at (-2,0) {$x$} ; %
%		    \node[latent] (y) at (2,0) {$y$} ; %
%		    \node[latent] (z) at (0,-2) {$z$} ; %
%		 \edge{x}{z};
%		 \edge{y}{z};
%		 }
%	}
%     \caption{v-structure}
%     \end{subfigure}
%        \caption{Three canonical graphs}
%        \label{fig:three graphs}
%\end{figure}
%
%\end{frame}

\begin{frame}{Three canonical graphs}


\begin{columns}[onlytextwidth,t]
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Cascade} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (0,0) {$X$} ; %
		    \node[latent] (y) at (0,-2) {$Y$} ; %
		    \node[latent] (z) at (0,-4) {$Z$} ; %
		 \edge{x}{y};
		 \edge{y}{z};
		 }
	} \\[.3cm]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Common parent} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (y) at (0,0) {$Y$} ; %
		    \node[latent] (x) at (-2,-4) {$X$} ; %
		    \node[latent] (z) at (2,-4) {$Z$} ; %
		 \edge{y}{x};
		 \edge{y}{z};
		 }
	}\\[.3cm] 
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[latent] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
    \end{column}
    
\end{columns}

\end{frame}


\begin{frame}{Three canonical graphs : Marginal Independence}


\begin{columns}[onlytextwidth,t]
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Cascade} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (0,0) {$X$} ; %
		    \node[latent] (y) at (0,-2) {$Y$} ; %
		    \node[latent] (z) at (0,-4) {$Z$} ; %
		 \edge{x}{y};
		 \edge{y}{z};
		 }
	} \\[.3cm]
	\[ Z \not\indep X \]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Common parent} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (y) at (0,0) {$Y$} ; %
		    \node[latent] (x) at (-2,-4) {$X$} ; %
		    \node[latent] (z) at (2,-4) {$Z$} ; %
		 \edge{y}{x};
		 \edge{y}{z};
		 }
	}\\[.3cm] 
	\[ Z \not\indep X \]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[latent] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
	\[ Z \indep X \]
    \end{column}
    
\end{columns}

\end{frame}



\begin{frame}{Three canonical graphs : Conditional Independence}


\begin{columns}[onlytextwidth,t]
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Cascade} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (0,0) {$X$} ; %
		    \node[obs] (y) at (0,-2) {$Y$} ; %
		    \node[latent] (z) at (0,-4) {$Z$} ; %
		 \edge{x}{y};
		 \edge{y}{z};
		 }
	} \\[.3cm]
	\[ Z \indep X \cond Y \]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Common parent} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[obs] (y) at (0,0) {$Y$} ; %
		    \node[latent] (x) at (-2,-4) {$X$} ; %
		    \node[latent] (z) at (2,-4) {$Z$} ; %
		 \edge{y}{x};
		 \edge{y}{z};
		 }
	}\\[.3cm] 
	\[ Z \indep X \cond Y \]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[obs] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
	\[ Z \not\indep X \cond Y \]
    \end{column}
    
\end{columns}

\end{frame}


\begin{frame}{Three canonical graphs : Take Home}


\begin{columns}[onlytextwidth,t]
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Cascade} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (0,0) {$X$} ; %
		    \node[obs] (y) at (0,-2) {$Y$} ; %
		    \node[latent] (z) at (0,-4) {$Z$} ; %
		 \edge{x}{y};
		 \edge{y}{z};
		 }
	} \\[.3cm]
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{Common parent} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[obs] (y) at (0,0) {$Y$} ; %
		    \node[latent] (x) at (-2,-4) {$X$} ; %
		    \node[latent] (z) at (2,-4) {$Z$} ; %
		 \edge{y}{x};
		 \edge{y}{z};
		 }
	}\\[.3cm] 
    \end{column}
    
 \begin{column}{0.33\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[obs] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
    \end{column}
    
\end{columns}

\vfill
\begin{columns}
 \begin{column}{0.67\textwidth}
 \centering
 Knowing $Y$ \alert{decouples} $X$ and $Z$
    \end{column}
  
   \begin{column}{0.33\textwidth}
   \centering
 Knowing $Y$ \\  \alert{couples} $X$ and $Z$
    \end{column}
    
 \end{columns}
 \end{frame}
 
 
 
 \begin{frame}{Competing explanations}




\begin{columns}[onlytextwidth,t]
 \begin{column}{0.3\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.7}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[obs] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
    \end{column}
    
 \begin{column}{0.6\textwidth}
 \scriptsize
The independence properties of the v-structure is commonly understood through a \alert{competing explanations} paradigm. \\[.3cm]
Suppose your house has a twitchy burglar alarm that is also sometimes triggered by earthquakes.    \\[.3cm]

Let 
\begin{align*}
 X &= \{ \text{your house got robbed} \} \\
 Z &= \{ \text{an earthquake occurred nearby} \} \\
  Y &= \{ \text{your burglar alarm goes off} \}  
 \end{align*}
 
 Then it is \tiny (perhaps) \scriptsize intuitive that
 \begin{align*}
   Z & \indep X  \\
   Z & \not\indep X \cond Y
 \end{align*}
 
  \end{column}
    
\end{columns}
\end{frame}

\begin{frame}{Relevance to real models}
\begin{columns}[onlytextwidth,t]
    
 \begin{column}{0.4\textwidth}
  \centering
    \textbf{Common parent} \\[.3cm]
		  \scalebox{0.4}{
		  \tikz{ %
		    \node[obs] (y) at (0,0) {$Y$} ; %
		    \node[latent] (x) at (-2,-4) {$X$} ; %
		    \node[latent] (z) at (2,-4) {$Z$} ; %
		 \edge{y}{x};
		 \edge{y}{z};
		 }
	}\\[.3cm] 
    \end{column}
    
 \begin{column}{0.4\textwidth}
  \centering
    \textbf{v-structure} \\[.3cm]
		  \scalebox{0.4}{
		  \tikz{ %
		    \node[latent] (x) at (-2,0) {$X$} ; %
		    \node[latent] (z) at (2,0) {$Z$} ; %
		    \node[obs] (y) at (0,-4) {$Y$} ; %
		 \edge{x}{y};
		 \edge{z}{y};
		 }
	} \\[.3cm]
    \end{column}
    
\end{columns}

In real models ...
\begin{itemize}
\item the \alert{common parent structure} shows up with conditional i.i.d data models. \tiny (So imagine $Y$ is a parameter and $X$ and $Z$ are two observations.)  \normalsize    The observations are conditionally independent, but integrating out the random parameter induces dependencies in the observations.   Note in particular that the observations are, in general, \textit{dependent} in the predictive posterior. %\footnote{Incidentally, this is why exchangeability is a weaker condition than independence.}   % SAY: The observations are conditionally independent, but integrating out the random parameter induces dependencies in the observations.
\item the \alert{v-structure} shows up with independent priors.   \tiny ( So imagine $X$ and $Z$ are  model parameters given independent priors and $Y$ is an observation.)  \normalsize  Then the parameters are independent when generating data (i.e. in the prior), but they become dependent when doing inference (i.e. in the posterior). 
\end{itemize}

\end{frame}

%\vfill
%\begin{columns}
% \begin{column}{0.67\textwidth}
%    \end{column}
%  
%   \begin{column}{0.33\textwidth}
%   \centering
% Knowing $Y$ \\  \alert{couples} $X$ and $Z$
%    \end{column}
%    
% \end{columns}

 
\section{Independence in Directed PGM's}


\begin{frame}{d-separation}

\begin{figure}[H]
\includegraphics[width=\textwidth]{images/d-separation}
%\caption{ \tiny Image Credit: Matt Gormley (CMU).}
%\label{fig:d_separation}
\end{figure} 
\vfill
\tiny \hfill Image Credit: Matt Gormley (CMU). 
 \end{frame}
 
 
\begin{frame}{Worksheet for practice}
\url{http://web.mit.edu/jmn/www/6.034/d-separation.pdf}
\end{frame}

\begin{frame}{Fundamental property of Bayes networks}

An oft-stated fact is:
\[ \textit{A node is independent of its non-descendants given its parents. } \]

This can easily be proven via d-separation.  
\begin{itemize}
\item The first step (``ancestral graph") will remove all of $X$'s children.
\item The fourth step (``remove givens") will remove $X$'s parents. 
\item Thus, $X$ will be disconnected from the rest of the graph.
\end{itemize}
\end{frame}


\begin{frame}{Markov blankets}
\begin{figure}[H]
\includegraphics[width=.8\textwidth]{images/markov_blanket_1}
%\caption{ \tiny Image Credit: Matt Gormley (CMU).}
%\label{fig:d_separation}
\end{figure} 
\vfill
\tiny \hfill Image Credit: Matt Gormley (CMU). 
 \end{frame}


\begin{frame}{Markov blankets}
\begin{figure}[H]
\includegraphics[width=.8\textwidth]{images/markov_blanket_2}
%\caption{ \tiny Image Credit: Matt Gormley (CMU).}
%\label{fig:d_separation}
\end{figure} 
\vfill
\tiny \hfill Image Credit: Matt Gormley (CMU). 
 \end{frame}

\begin{frame}{Markov Blankets: Why \textit{co}-parents?}

Why is it not sufficient for the Markov Blanket to only include the parents and children of $X_i$? \\
\pause
\vfill
The phenomenon of explaining away means that the observations of child nodes will not block paths to the co-parents. \\
\vfill
This is why step 2 of the d-separation algorithm ("moralization") connects parents. \\
\vfill
In the previous graph, the transformed graph would still have paths from $X_6$ to, for example, $X_8$ (and to $X_{11}$).
\end{frame}

\begin{frame}{Proof of Markov Blanket statement}
Let us consider the conditional distribution of some variable $X_i$ given the factorization in \eqref{eqn:joint_factorization}:

\begin{align*}
 p(X_i  \cond X_{-i}) &= \df{ p(X_1, ..., X_n)}{ \ds\int  p(X_1, ..., X_n) d\, X_i} \\
 &= \df{\ds\prod_{k=1}^n p(X_k \cond \pi_k)}{\ds\int \ds\prod_{k=1}^n p(X_k \cond \pi_k) \; d\,X_i }
 \end{align*}
 
All terms will cancel in the numerator and denominator except for terms of the form
\begin{enumerate}
\item $p(X_i \cond \pi_i)$, i.e. terms where $i$ is the node itself
\item $\{p(X_k \cond \pi_k) : i \in \pi_k \} $, i.e. terms where $i$ is one of the parents.
\end{enumerate}

Terms of type (1) will depend on $X_i$'s parents, and terms of type (2) will depend on $X_i$'s children and co-parents.

\end{frame}
 

\end{document}