\pdfoutput=1
\documentclass[10pt]{beamer}

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{../../rsrc/beamer_preamble}

% EMBED ANIMATIONS
% Reference: https://tex.stackexchange.com/questions/240243/getting-gif-and-or-moving-images-into-a-latex-presentation
%\usepackage{xmpmulti}
\usepackage{animate}

%%% SPECIFIC TO THIS DOC
\newcommand{\obs}{x^{(i)} }
\newcommand{\alatent}{z^{(i)} }
\newcommand{\I}{\mathbb{I}}





\title{Normalizing Flows}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Overview}
 
\begin{frame}{Main Idea}
  \animategraphics[loop,controls,width=\linewidth]{10}{movie_frames/two_moons-}{1}{30}

\scriptsize
\textit{Normalizing flows} provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of
bijective transformations.
\vfill
\tiny \hfill Animation Credit: Mike Slawinski, Ph.D., Cylance, Inc.
\end{frame}
    
\begin{frame}{}

\begin{figure}[H]
\includegraphics[width=\textwidth]{images/nf_idea}
\end{figure} 
We learn an invertible mapping between a data distribution $\widehat{p}_X$ and a latent distribution $p_Z$ (typically a Gaussian).  The technique allows one to perform anomaly detection and sample generation even for complex and high-dimensional distributions.
\vfill
\tiny \hfill Image Credit: Dinh et al. (2017), ICLR. 
 \end{frame}
\section{Density estimation and change of variables} 



\begin{frame}
Consider a parametric function mapping continuous random variable $X$ to continuous random variable $Z$

\begin{align*}
f_\theta:  &\, X \to Z \\
			&\, x \mapsto z 
\end{align*}

where $x$ is an observed sample and $\+z$ is a latent variable.  Suppose $p_Z$ is given.   Then by the change of variables theorem, we have\footnote{assuming the conditions of the theorem are met}

\[ \labeledgreenbox{data \; space}{$p_X(x)$} = \labeledbluebox{transformed \; latent \; space}{$p_Z( f_\theta(x) ) \absdet{\df{\partial f_\theta(x)}{\partial x}}$}  \]  
\end{frame}

\begin{frame}
%\red{Should I transpose x here and elsewhere?} 
The goal of density estimation can be posed as follows: learn $\theta$ to model unknown data density $p_X$ in terms of assumed latent variable density $p_Z$.  
\end{frame}

\section{Normalizing Flow} 

\begin{frame}
\begin{definition}{(Normalizing Flow)} \label{flow}
A \textit{(normalizing) flow}, $f=h_\theta^1 \circ ... \circ h_\theta^K$, is a sequence of invertible transformations which maps an observed data point, $x$, to a latent state representation, $z$. 
\end{definition}

If we allow ourselves this abuse of notation\footnote{More generally, we will use the same notation to refer to the function itself as well as its evaluation at a point.}
\begin{align*}
h_\theta^0 &:=x \\
h_\theta^K &:= z \\
\end{align*}

Then, since $\det \prod_i A_i = \prod_i \det A_i$, the likelihood becomes 

\begin{equation} 
\label{cov}
p_X(x) = p_Z( f_\theta(x) ) \ds\prod_{k=1}^K\absdet{\df{\partial h_\theta^k}{\partial h_\theta^{k-1}}}  
\end{equation}
\end{frame}

%A flow-based generative model assumes $p_\theta(z)$ is tractable, e.g. a spherical multivariate Gaussian.  Then, by the change of variables formula, we can use $f$ and its inverse $g := f^{-1}$ to compute the probability density $p_\theta(x)$.   Training the model means learning $\theta$ to maximize the likelihood
%
%\[ p_X(x) = p_Z(z) \absdet{\df{\partial f(x)}{\partial x^T}}  \] 
%
%where we have used the change of variables to express density $p_X$ in terms of density $p_Z$.  


%\begin{definition}{(Coupling layer)} \label{coupling_layer}
%\red{to do}
%\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real NVP}

\begin{definition}{(Real NVP)} \label{rnvp}
A \textit{real NVP} is a normalizing flow (Def. \ref{flow}) where $f=h_\theta^1 \circ ... \circ h_\theta^K$ is structured such that:

\[h^{i+1} = b^i \odot h^i + (1-b^i) \odot \bigg( h^i \odot \exp \big( s^i_\theta(b^i \odot h^i) \big) + t^i_\theta(b^i \odot h^i) \bigg)  \]
 
where $b^1, ..., b^K$ is a sequence of binary masks, $\odot$ is the Hadamard product or element-wise product, and $s$ and $t$ stand for scale and translation. %\red{I'm still following their notational suckiness of conflating a function with the output of a function}
\end{definition}


\begin{definition}{(Affine Coupling Layer)} \label{acl}
An affine coupling layer is one element of the sequence of invertible transformations in a real NVP; i.e. it is $h_i$ for some $i \in \set{1,...,K}$ in Def. \ref{rnvp}. 
\end{definition}

\begin{example}\label{rnvp_example}

If random variables are $D$ dimensional, and $b^i := [1,... 1, 0,...0]$, where the 0 entries begin at the $d_{i+1}$st element, then the affine coupling layer is given by

\begin{align*}
h^{i+1}_{1:d_i} &= h^i_{1:d_i} \\
h^{i+1}_{d_i+1:D} &= h^i_{d_i+1:D} \odot \exp \big( s^i_\theta(h^i_{1:d_i}) \big) + t^i_\theta(h^i_{1:d_i})  \\
\end{align*}


\end{example}


\begin{frame}
Note that the real NVP allows for efficient computation of the determinant of the Jacobians, since 

\[ \df{\partial h_\theta^{i+1}}{\partial h_\theta^{i}} = 
	\begin{pmatrix}
	\I_d & 0\\ 
	\df{\partial h_{d_i+1:D}^{i+1}}{\partial h_{1:d_i}^{i}} & \text{diag} \bigg( \exp \big( s_\theta (h^i_{1:d_i}) \big) \bigg) 
	\end{pmatrix}
\]

The bottom left term can be arbitrarily complex; we don't have to compute it, since the determinant of a triangular matrix is the product of the diagonals:

\[ \det \df{\partial h_\theta^{i+1}}{\partial h_\theta^{i}} = \exp \bigg( \ds\sum_j s_\theta^i \, \big( h^i_{1:d_i} \big)_j \bigg) \] 
\end{frame}

% Needs work. 
%Thus, the log likelihood for this single step in the transformation, applied to a single data point, $h^i$, is
%
%\[ \log p_X(h^i) = \log p_Z( h^{i+1}_\theta(h^i) ) + \ds\sum_j s_\theta^i \, \big( h^i_{1:d_i} \big)_j \] 

\begin{frame}
So, by Equation \ref{cov}, the log likelihood with real NVP normalizing flow applied to a single data sample, $x$, is

\[ \log p_X(x) = \log p_Z( f_\theta(x) ) + \explainterm{\blue{(LAYERS)}}{\ds\sum_i} \explainterm{\blue{(FEATURES)}}{\ds\sum_j} s_\theta^i \, \big( h^i_{1:d_i} \big)_j \] 


And the log likelihood applied for a collection of samples, assumed \textit{i.i.d.}, is the sum of individual log likelihoods. 
\end{frame}
 

\end{document}