# Statistical Machine Learning for Anomaly Detection 

## Instructors 
Mike Wojnowicz and Karin Knudson 


## Goal 
This course will introduce students to a variety of models in statistical machine learning that can be used for anomaly detection. Topics will include:  Mixture Models, Hidden Markov Models, Latent Dirichlet Allocation, Variational Autoencoders, and Normalizing Flows. Areas of application range from cybersecurity to biology to speech recognition. 

## Pre-requisites

Students should have some passing familiarity with basic statistical concepts (maximum likelihood, Bayes law, expectations, conditional probability), and should be comfortable with Python programming.

## Structure 

We break the course into four topics, one per day.  (The last day is a flex day.)  Each day in turn has four subtopics.  Each subtopic will consist of about 20 minutes of presentation, followed by about 20 minutes of interactive group activity.   

## Topics

* Day 1: Statistical Inference / (maybe) Ice Breaker / (maybe) Python Template 
    * Distribution Fitting via Maximum Likelihood 
    * Bayesian approaches and Conjugate Bayesian Models
    * Exponential Families 
    * Probabilistic Graphical Models
* Day 2: Expectation Maximization (EM) 
    * Overview
    * Mixture Models
    * Hidden Markov Models
    * Cybersecurity:  Malware Ground Truth 
* Day 3: Variational Inference (VI)
    * Overview (and Relation to EM) 
    * Bayesian Mixture Models
    * Bayesian Hidden Markov Models
    * Latent Dirichlet Allocation 
* Day 4: Black Box Models 
    * Introduction (Motivation, PGM's + NN's, etc.) 
    * Automatic Differentiation Variational Inference
    * VAE 
    * Normalizing Flow
* Day 5: Flex Day 


## Further Readings

### Day 1: Overview 
* Statistical Inference
   * Casella, G., & Berger, R. L. (2002). Statistical inference (Vol. 2, pp. 337-472). Pacific Grove, CA: Duxbury. 
* Introduction to Bayesian Modeling
	* Hoff, P. D. (2009). A first course in Bayesian statistical methods (Vol. 580). New York: Springer.
	* Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press
* Graphical Models
	* CH 8 of Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
	* Ghahramani, Z. (2001). An introduction to hidden Markov models and Bayesian networks. In Hidden Markov models: applications in computer vision (pp. 9-41).
* Exponential Families
	* Michael I Jordan's chapter: https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf
	* Michael I Jordan's chapter: https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf

### Day 2: EM
* Mixture Models and EM  
	* CH 9 of Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
* HMM
	* Sec 13.2 of Bishop, C. M. (2006). Pattern recognition and machine learning. springer.	
	* Ghahramani, Z. (2001). An introduction to hidden Markov models and Bayesian networks. In Hidden Markov models: applications in computer vision (pp. 9-41).

	
### Day 3: Variational Inference
* Introduction to VI 
	* Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518), 859-877.
* Latent Dirichlet Allocation
	* Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.

 	
